08/28/2022 02:14:32 - WARNING - run_glue_f1_macro -   Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: False
08/28/2022 02:14:32 - INFO - run_glue_f1_macro -   Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=True,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_02-14-32_e936dc671815,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=f1,
mp_parameters=,
no_cuda=False,
num_train_epochs=9.0,
output_dir=./checkpoints,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=./checkpoints,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=3,
seed=26,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2022 02:14:32 - INFO - run_glue_f1_macro -   load a local file for train: ./tmp/train.csv
08/28/2022 02:14:32 - INFO - run_glue_f1_macro -   load a local file for validation: ./tmp/dev.csv
08/28/2022 02:14:33 - WARNING - datasets.builder -   Using custom data configuration default-222870d4cba3ac4c
08/28/2022 02:14:33 - WARNING - datasets.builder -   Reusing dataset csv (/home/.cache/huggingface/datasets/csv/default-222870d4cba3ac4c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)
  0%|                                                                                                                                               | 0/2 [00:00<?, ?it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 892.31it/s]
[INFO|configuration_utils.py:515] 2022-08-28 02:14:33,184 >> loading configuration file ../../local_models/bert-base-uncased_SequenceClassification_STR1_0.1_mirror/config.json
[INFO|configuration_utils.py:553] 2022-08-28 02:14:33,184 >> Model config BertConfig {
  "_name_or_path": "../local_models/bert-base-uncased_0.1_MaskedLM_STR1_0.1_mirror",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 32260
}

[INFO|configuration_utils.py:515] 2022-08-28 02:14:33,184 >> loading configuration file ../../local_models/bert-base-uncased_SequenceClassification_STR1_0.1_mirror/config.json
[INFO|configuration_utils.py:553] 2022-08-28 02:14:33,185 >> Model config BertConfig {
  "_name_or_path": "../local_models/bert-base-uncased_0.1_MaskedLM_STR1_0.1_mirror",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 32260
}

[INFO|tokenization_utils_base.py:1715] 2022-08-28 02:14:33,185 >> loading file ../../local_models/bert-base-uncased_SequenceClassification_STR1_0.1_mirror/vocab.txt
[INFO|tokenization_utils_base.py:1715] 2022-08-28 02:14:33,185 >> loading file ../../local_models/bert-base-uncased_SequenceClassification_STR1_0.1_mirror/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2022-08-28 02:14:33,185 >> loading file ../../local_models/bert-base-uncased_SequenceClassification_STR1_0.1_mirror/added_tokens.json
[INFO|tokenization_utils_base.py:1715] 2022-08-28 02:14:33,185 >> loading file ../../local_models/bert-base-uncased_SequenceClassification_STR1_0.1_mirror/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2022-08-28 02:14:33,185 >> loading file ../../local_models/bert-base-uncased_SequenceClassification_STR1_0.1_mirror/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2022-08-28 02:15:15,738 >> loading weights file ../../local_models/bert-base-uncased_SequenceClassification_STR1_0.1_mirror/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2022-08-28 02:15:16,849 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:1344] 2022-08-28 02:15:16,849 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../../local_models/bert-base-uncased_SequenceClassification_STR1_0.1_mirror.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
08/28/2022 02:15:16 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/.cache/huggingface/datasets/csv/default-222870d4cba3ac4c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-a41962d7cc943599.arrow
  0%|                                                                                                                                               | 0/5 [00:00<?, ?ba/s] 20%|███████████████████████████                                                                                                            | 1/5 [00:00<00:03,  1.12ba/s] 40%|██████████████████████████████████████████████████████                                                                                 | 2/5 [00:02<00:03,  1.03s/ba] 60%|█████████████████████████████████████████████████████████████████████████████████                                                      | 3/5 [00:04<00:03,  1.67s/ba] 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                           | 4/5 [00:06<00:01,  1.96s/ba]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.94s/ba]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.75s/ba]
08/28/2022 02:15:25 - INFO - run_glue_f1_macro -   Sample 13287 of the training set: {'sentence': 'Far from finding hostility she felt people were very receptive , and her second in command was so competent Eva continued to puzzle over her own sideways move . It must have been a delicate situation . Here was someone in the top position virtually starting <START> IDfromscratchID <END> as far as the day - to - day running of the various institutions were concerned , When she tried to lay down the law experienced officers found it hard to stomach . Nevertheless they swallowed their wounded pride and did all they could to help if she asked for advice . " She just went out of her way to get to know what was going on . "', 'label': 0, 'input_ids': [101, 2521, 2013, 4531, 18258, 2016, 2371, 2111, 2020, 2200, 28667, 22048, 1010, 1998, 2014, 2117, 1999, 3094, 2001, 2061, 17824, 9345, 2506, 2000, 11989, 2058, 2014, 2219, 12579, 2693, 1012, 2009, 2442, 2031, 2042, 1037, 10059, 3663, 1012, 2182, 2001, 2619, 1999, 1996, 2327, 2597, 8990, 3225, 1026, 2707, 1028, 30633, 1026, 2203, 1028, 2004, 2521, 2004, 1996, 2154, 1011, 2000, 1011, 2154, 2770, 1997, 1996, 2536, 4896, 2020, 4986, 1010, 2043, 2016, 2699, 2000, 3913, 2091, 1996, 2375, 5281, 3738, 2179, 2009, 2524, 2000, 4308, 1012, 6600, 2027, 7351, 2037, 5303, 6620, 1998, 2106, 2035, 2027, 2071, 2000, 2393, 2065, 2016, 2356, 2005, 6040, 1012, 1000, 2016, 2074, 2253, 2041, 1997, 2014, 2126, 2000, 2131, 2000, 2113, 2054, 2001, 2183, 2006, 1012, 1000, 102, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}.
08/28/2022 02:15:25 - INFO - run_glue_f1_macro -   Sample 13445 of the training set: {'sentence': 'The only meaningful economy here now operates in US dollars . With them you can buy just about whatever your little capitalist heart desires . Without , with a rouble paycheck or , worse still , a rouble pension , life is slipping inexorably <START> IDdownthetubesID <END> . The signs that it is doing so are around you in ways which were unimaginable before prices were ‘ liberalised’ two years ago . Little urchins tugging on our coat tails , miming eating , asking for money like some white - skinned version of a miniature beggar in a Calcutta slum .', 'label': 0, 'input_ids': [101, 1996, 2069, 15902, 4610, 2182, 2085, 5748, 1999, 2149, 6363, 1012, 2007, 2068, 2017, 2064, 4965, 2074, 2055, 3649, 2115, 2210, 19640, 2540, 14714, 1012, 2302, 1010, 2007, 1037, 20996, 12083, 2571, 3477, 5403, 3600, 2030, 1010, 4788, 2145, 1010, 1037, 20996, 12083, 2571, 11550, 1010, 2166, 2003, 11426, 1999, 10288, 6525, 6321, 1026, 2707, 1028, 31183, 1026, 2203, 1028, 1012, 1996, 5751, 2008, 2009, 2003, 2725, 2061, 2024, 2105, 2017, 1999, 3971, 2029, 2020, 4895, 9581, 20876, 3468, 2077, 7597, 2020, 1520, 4314, 5084, 1521, 2048, 2086, 3283, 1012, 2210, 24471, 17231, 2015, 17100, 2006, 2256, 5435, 17448, 1010, 20705, 3070, 5983, 1010, 4851, 2005, 2769, 2066, 2070, 2317, 1011, 19937, 2544, 1997, 1037, 12973, 11693, 6843, 1999, 1037, 13419, 22889, 2819, 1012, 102, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}.
08/28/2022 02:15:25 - INFO - run_glue_f1_macro -   Sample 28341 of the training set: {'sentence': "Comfort put down her wine glass with a snap , her smile shrinking like a jersey boiled in washing soda . ‘ Julia is not dull,’ she said through her teeth . ‘ Oh my!’ said Bob , looking at her more seriously than his voice would have suggested , ‘ you have <START> IDchangeyourtuneID <END> . What 's happened?’ ‘ Nothing,’ said Comfort , who had been unable either to forget or justify some of the things she had said to Julia during the war .", 'label': 0, 'input_ids': [101, 7216, 2404, 2091, 2014, 4511, 3221, 2007, 1037, 10245, 1010, 2014, 2868, 28375, 2066, 1037, 3933, 17020, 1999, 12699, 14904, 1012, 1520, 6423, 2003, 2025, 10634, 1010, 1521, 2016, 2056, 2083, 2014, 4091, 1012, 1520, 2821, 2026, 999, 1521, 2056, 3960, 1010, 2559, 2012, 2014, 2062, 5667, 2084, 2010, 2376, 2052, 2031, 4081, 1010, 1520, 2017, 2031, 1026, 2707, 1028, 30761, 1026, 2203, 1028, 1012, 2054, 1005, 1055, 3047, 1029, 1521, 1520, 2498, 1010, 1521, 2056, 7216, 1010, 2040, 2018, 2042, 4039, 2593, 2000, 5293, 2030, 16114, 2070, 1997, 1996, 2477, 2016, 2018, 2056, 2000, 6423, 2076, 1996, 2162, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
[INFO|trainer.py:1047] 2022-08-28 02:15:27,626 >> Loading model from ./checkpoints/checkpoint-5445).
[INFO|trainer.py:514] 2022-08-28 02:15:28,001 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.
[INFO|trainer.py:1147] 2022-08-28 02:15:28,730 >> ***** Running training *****
[INFO|trainer.py:1148] 2022-08-28 02:15:28,731 >>   Num examples = 38715
[INFO|trainer.py:1149] 2022-08-28 02:15:28,731 >>   Num Epochs = 9
[INFO|trainer.py:1150] 2022-08-28 02:15:28,731 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2022-08-28 02:15:28,731 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2022-08-28 02:15:28,731 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1153] 2022-08-28 02:15:28,731 >>   Total optimization steps = 5445
[INFO|trainer.py:1173] 2022-08-28 02:15:28,731 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:1174] 2022-08-28 02:15:28,731 >>   Continuing training from epoch 9
[INFO|trainer.py:1175] 2022-08-28 02:15:28,731 >>   Continuing training from global step 5445
[INFO|trainer.py:1177] 2022-08-28 02:15:28,731 >>   Will skip the first 9 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
0it [00:00, ?it/s]Skipping the first batches: : 0it [00:00, ?it/s]
  0%|                                                                                                                                            | 0/5445 [00:00<?, ?it/s][A[INFO|trainer.py:1343] 2022-08-28 02:15:28,901 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1351] 2022-08-28 02:15:28,901 >> Loading best model from ./checkpoints/checkpoint-3025 (score: 0.6850035536602701).
                                                
                                                                                                                                                                          [ASkipping the first batches: : 0it [00:00, ?it/s]
  0%|                                                                                                                                            | 0/5445 [00:00<?, ?it/s][A  0%|                                                                                                                                            | 0/5445 [00:00<?, ?it/s]
Skipping the first batches: : 0it [00:00, ?it/s]
[INFO|trainer.py:1894] 2022-08-28 02:15:29,178 >> Saving model checkpoint to ./checkpoints
[INFO|configuration_utils.py:351] 2022-08-28 02:15:29,179 >> Configuration saved in ./checkpoints/config.json
[INFO|modeling_utils.py:886] 2022-08-28 02:15:30,120 >> Model weights saved in ./checkpoints/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2022-08-28 02:15:30,121 >> tokenizer config file saved in ./checkpoints/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2022-08-28 02:15:30,121 >> Special tokens file saved in ./checkpoints/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2022-08-28 02:15:30,177 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:30,177 >>   epoch                    =        9.0
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:30,177 >>   train_loss               =        0.0
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:30,177 >>   train_runtime            = 0:00:00.44
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:30,177 >>   train_samples            =      38715
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:30,177 >>   train_samples_per_second = 788835.454
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:30,177 >>   train_steps_per_second   =  12327.146
{'train_runtime': 0.4417, 'train_samples_per_second': 788835.454, 'train_steps_per_second': 12327.146, 'train_loss': 0.0, 'epoch': 9.0}
08/28/2022 02:15:30 - INFO - run_glue_f1_macro -   *** Evaluate ***
[INFO|trainer.py:514] 2022-08-28 02:15:30,179 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.
[INFO|trainer.py:2140] 2022-08-28 02:15:30,181 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2022-08-28 02:15:30,182 >>   Num examples = 4840
[INFO|trainer.py:2145] 2022-08-28 02:15:30,182 >>   Batch size = 64
/opt/conda/envs/acp20mym-idiomprinciple/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|                                                                                                                                              | 0/76 [00:00<?, ?it/s]  3%|███▌                                                                                                                                  | 2/76 [00:00<00:05, 13.08it/s]  5%|███████                                                                                                                               | 4/76 [00:00<00:08,  8.27it/s]  7%|████████▊                                                                                                                             | 5/76 [00:00<00:09,  7.67it/s]  8%|██████████▌                                                                                                                           | 6/76 [00:00<00:09,  7.31it/s]  9%|████████████▎                                                                                                                         | 7/76 [00:00<00:09,  7.04it/s] 11%|██████████████                                                                                                                        | 8/76 [00:01<00:09,  6.89it/s] 12%|███████████████▊                                                                                                                      | 9/76 [00:01<00:09,  6.79it/s] 13%|█████████████████▌                                                                                                                   | 10/76 [00:01<00:09,  6.72it/s] 14%|███████████████████▎                                                                                                                 | 11/76 [00:01<00:09,  6.67it/s] 16%|█████████████████████                                                                                                                | 12/76 [00:01<00:09,  6.61it/s] 17%|██████████████████████▊                                                                                                              | 13/76 [00:01<00:09,  6.60it/s] 18%|████████████████████████▌                                                                                                            | 14/76 [00:01<00:09,  6.59it/s] 20%|██████████████████████████▎                                                                                                          | 15/76 [00:02<00:09,  6.58it/s] 21%|████████████████████████████                                                                                                         | 16/76 [00:02<00:09,  6.57it/s] 22%|█████████████████████████████▊                                                                                                       | 17/76 [00:02<00:08,  6.56it/s] 24%|███████████████████████████████▌                                                                                                     | 18/76 [00:02<00:08,  6.56it/s] 25%|█████████████████████████████████▎                                                                                                   | 19/76 [00:02<00:08,  6.56it/s] 26%|███████████████████████████████████                                                                                                  | 20/76 [00:02<00:08,  6.56it/s] 28%|████████████████████████████████████▊                                                                                                | 21/76 [00:03<00:08,  6.57it/s] 29%|██████████████████████████████████████▌                                                                                              | 22/76 [00:03<00:08,  6.57it/s] 30%|████████████████████████████████████████▎                                                                                            | 23/76 [00:03<00:08,  6.57it/s] 32%|██████████████████████████████████████████                                                                                           | 24/76 [00:03<00:07,  6.57it/s] 33%|███████████████████████████████████████████▊                                                                                         | 25/76 [00:03<00:07,  6.56it/s] 34%|█████████████████████████████████████████████▌                                                                                       | 26/76 [00:03<00:07,  6.56it/s] 36%|███████████████████████████████████████████████▎                                                                                     | 27/76 [00:03<00:07,  6.57it/s] 37%|█████████████████████████████████████████████████                                                                                    | 28/76 [00:04<00:07,  6.57it/s] 38%|██████████████████████████████████████████████████▊                                                                                  | 29/76 [00:04<00:07,  6.56it/s] 39%|████████████████████████████████████████████████████▌                                                                                | 30/76 [00:04<00:07,  6.56it/s] 41%|██████████████████████████████████████████████████████▎                                                                              | 31/76 [00:04<00:06,  6.56it/s] 42%|████████████████████████████████████████████████████████                                                                             | 32/76 [00:04<00:06,  6.55it/s] 43%|█████████████████████████████████████████████████████████▊                                                                           | 33/76 [00:04<00:07,  5.80it/s] 45%|███████████████████████████████████████████████████████████▌                                                                         | 34/76 [00:05<00:06,  6.01it/s] 46%|█████████████████████████████████████████████████████████████▎                                                                       | 35/76 [00:05<00:06,  6.17it/s] 47%|███████████████████████████████████████████████████████████████                                                                      | 36/76 [00:05<00:06,  6.28it/s] 49%|████████████████████████████████████████████████████████████████▊                                                                    | 37/76 [00:05<00:06,  6.36it/s] 50%|██████████████████████████████████████████████████████████████████▌                                                                  | 38/76 [00:05<00:05,  6.42it/s] 51%|████████████████████████████████████████████████████████████████████▎                                                                | 39/76 [00:05<00:05,  6.46it/s] 53%|██████████████████████████████████████████████████████████████████████                                                               | 40/76 [00:06<00:05,  6.49it/s] 54%|███████████████████████████████████████████████████████████████████████▊                                                             | 41/76 [00:06<00:05,  6.49it/s] 55%|█████████████████████████████████████████████████████████████████████████▌                                                           | 42/76 [00:06<00:05,  6.51it/s] 57%|███████████████████████████████████████████████████████████████████████████▎                                                         | 43/76 [00:06<00:05,  6.52it/s] 58%|█████████████████████████████████████████████████████████████████████████████                                                        | 44/76 [00:06<00:04,  6.53it/s] 59%|██████████████████████████████████████████████████████████████████████████████▊                                                      | 45/76 [00:06<00:04,  6.54it/s] 61%|████████████████████████████████████████████████████████████████████████████████▌                                                    | 46/76 [00:06<00:04,  6.55it/s] 62%|██████████████████████████████████████████████████████████████████████████████████▎                                                  | 47/76 [00:07<00:04,  6.53it/s] 63%|████████████████████████████████████████████████████████████████████████████████████                                                 | 48/76 [00:07<00:04,  6.54it/s] 64%|█████████████████████████████████████████████████████████████████████████████████████▊                                               | 49/76 [00:07<00:04,  6.54it/s] 66%|███████████████████████████████████████████████████████████████████████████████████████▌                                             | 50/76 [00:07<00:03,  6.55it/s] 67%|█████████████████████████████████████████████████████████████████████████████████████████▎                                           | 51/76 [00:07<00:03,  6.55it/s] 68%|███████████████████████████████████████████████████████████████████████████████████████████                                          | 52/76 [00:07<00:03,  6.56it/s] 70%|████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 53/76 [00:07<00:03,  6.55it/s] 71%|██████████████████████████████████████████████████████████████████████████████████████████████▌                                      | 54/76 [00:08<00:03,  6.56it/s] 72%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                    | 55/76 [00:08<00:03,  6.55it/s] 74%|██████████████████████████████████████████████████████████████████████████████████████████████████                                   | 56/76 [00:08<00:03,  6.56it/s] 75%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                                 | 57/76 [00:08<00:02,  6.55it/s] 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 58/76 [00:08<00:02,  6.55it/s] 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎                             | 59/76 [00:08<00:02,  6.55it/s] 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 60/76 [00:09<00:02,  6.55it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 61/76 [00:09<00:02,  6.54it/s] 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 62/76 [00:09<00:02,  6.55it/s] 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                      | 63/76 [00:09<00:01,  6.55it/s] 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 64/76 [00:09<00:01,  6.55it/s] 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 65/76 [00:09<00:01,  6.55it/s] 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 66/76 [00:09<00:01,  6.55it/s] 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 67/76 [00:10<00:01,  6.55it/s] 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████              | 68/76 [00:10<00:01,  6.55it/s] 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 69/76 [00:10<00:01,  6.55it/s] 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 70/76 [00:10<00:00,  6.55it/s] 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 71/76 [00:10<00:00,  6.55it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 72/76 [00:10<00:00,  6.56it/s] 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 73/76 [00:11<00:00,  6.56it/s] 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 74/76 [00:11<00:00,  6.56it/s] 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 75/76 [00:11<00:00,  6.56it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76/76 [00:11<00:00,  7.09it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76/76 [00:11<00:00,  6.62it/s]
[INFO|trainer_pt_utils.py:908] 2022-08-28 02:15:44,759 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:44,759 >>   epoch                   =        9.0
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:44,759 >>   eval_accuracy           =     0.7711
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:44,760 >>   eval_f1                 =      0.685
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:44,760 >>   eval_loss               =     1.0993
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:44,760 >>   eval_runtime            = 0:00:14.57
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:44,760 >>   eval_samples            =       4840
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:44,760 >>   eval_samples_per_second =    332.021
[INFO|trainer_pt_utils.py:913] 2022-08-28 02:15:44,760 >>   eval_steps_per_second   =      5.214
